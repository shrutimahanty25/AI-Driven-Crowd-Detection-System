{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6430589-2547-4794-97db-98de25564b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision opencv-python numpy matplotlib h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96579d3d-ee4e-4df2-8c4b-fc91a1c84cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import CSRNet\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CSRNet().to(device)\n",
    "\n",
    "# Safely load full checkpoint file\n",
    "checkpoint = torch.load(\"partBmodel_best.pth\", map_location=device, weights_only=False)\n",
    "\n",
    "# Check what's inside\n",
    "if 'state_dict' in checkpoint:\n",
    "    print(\"‚úî Found 'state_dict' in checkpoint ‚Äî loading it...\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "else:\n",
    "    print(\"‚ö† No 'state_dict' found ‚Äî assuming raw weights dict...\")\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1194e3db-59ff-48d5-8c45-cd546170b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8fe94-2c7d-44da-a2ef-1e2cf36e7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74ef53-d90e-4f54-863e-df84b4b46452",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets tqdm --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d5ec6-ef1d-4182-8e06-7247fec11f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde90c5f-60e5-4c9f-83cb-78dd0ddb0553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# === 1. Set device and load model ===\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CSRNet().to(device)\n",
    "\n",
    "# Load checkpoint (adjust path if needed)\n",
    "checkpoint = torch.load(\"partBmodel_best.pth\", map_location=device, weights_only=False)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# === 2. Preprocessing for each frame ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# === 3. Load Video ===\n",
    "input_path = r\"C:\\Users\\shrut\\Downloads\\Video\\trim\\trim3.mp4\"\n",
    "# output_path = r\"C:\\Users\\shrut\\Downloads\\new_leeyeehoo_model\\output_trim4.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "# out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "print(\"Processing video...\")\n",
    "for _ in tqdm(range(frame_count)):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # === 4. Frame Preprocessing ===\n",
    "    # img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # img_pil = Image.fromarray(img_rgb)\n",
    "    # img_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    # === 5. Forward pass ===\n",
    "    # with torch.no_grad():\n",
    "    #     density_map = model(img_tensor)\n",
    "    #     density_np = density_map.squeeze().cpu().numpy()\n",
    "    #     count = int(density_np.sum())\n",
    "\n",
    "    def count_frame_patched(frame, model, transform, patch_size=(384, 384)):\n",
    "    H, W, _ = frame.shape\n",
    "    patch_h, patch_w = patch_size\n",
    "    total_count = 0\n",
    "\n",
    "    for y in range(0, H, patch_h):\n",
    "        for x in range(0, W, patch_w):\n",
    "            patch = frame[y:min(y+patch_h, H), x:min(x+patch_w, W)]\n",
    "            patch_resized = cv2.resize(patch, patch_size)\n",
    "            patch_rgb = cv2.cvtColor(patch_resized, cv2.COLOR_BGR2RGB)\n",
    "            patch_pil = Image.fromarray(patch_rgb)\n",
    "            input_tensor = transform(patch_pil).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                density_map = model(input_tensor)\n",
    "                count = float(density_map.sum().item())\n",
    "                total_count += count\n",
    "\n",
    "    return int(total_count)\n",
    "\n",
    "    # === 5. Patched forward pass ===\n",
    "    count = count_frame_patched(frame, model, transform)\n",
    "\n",
    "\n",
    "\n",
    "    # === 6. Draw count ===\n",
    "    cv2.putText(frame, f\"Count: {count}\", (30, 60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.8, (0, 0, 255), 3)\n",
    "\n",
    "    # === 7. Live display + write output ===\n",
    "    cv2.imshow('Live Headcount', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # Press Esc to stop early\n",
    "        break\n",
    "\n",
    "    # out.write(frame)\n",
    "\n",
    "\n",
    "# === 8. Cleanup ===\n",
    "cap.release()\n",
    "# out.release()\n",
    "# print(\"‚úÖ Video saved at:\", output_path)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3d664-8978-4749-b898-eaa92b79a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === 1. Define CSRNet model (reuse your existing model.py) ===\n",
    "from model import CSRNet\n",
    "\n",
    "# === 2. Load model ===\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CSRNet().to(device)\n",
    "\n",
    "checkpoint = torch.load(\"partBmodel_best.pth\", map_location=device, weights_only=False)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# === 3. Define transform ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# === 4. Patch-based CSRNet counting ===\n",
    "def count_frame_patched(frame, model, transform, patch_size=(384, 384)):\n",
    "    H, W, _ = frame.shape\n",
    "    patch_h, patch_w = patch_size\n",
    "    total_count = 0\n",
    "\n",
    "    for y in range(0, H, patch_h):\n",
    "        for x in range(0, W, patch_w):\n",
    "            patch = frame[y:min(y+patch_h, H), x:min(x+patch_w, W)]\n",
    "            patch_resized = cv2.resize(patch, patch_size)\n",
    "            patch_rgb = cv2.cvtColor(patch_resized, cv2.COLOR_BGR2RGB)\n",
    "            patch_pil = Image.fromarray(patch_rgb)\n",
    "            input_tensor = transform(patch_pil).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                density_map = model(input_tensor)\n",
    "                count = float(density_map.sum().item())\n",
    "                total_count += count\n",
    "\n",
    "    return int(total_count)\n",
    "\n",
    "# === 5. Load video ===\n",
    "input_path = r\"C:\\Users\\shrut\\Downloads\\Video\\trim\\maingate.mp4\"\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# === 6. Process and display ===\n",
    "print(\"üîç Live processing... Press ESC to quit.\")\n",
    "\n",
    "frame_skip = 5\n",
    "\n",
    "for i in tqdm(range(0, frame_count, frame_skip)):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "# for _ in tqdm(range(frame_count)):\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "    count = count_frame_patched(frame, model, transform)\n",
    "\n",
    "    # Overlay count\n",
    "    cv2.putText(frame, f\"Count: {count}\", (30, 60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.8, (0, 0, 255), 3)\n",
    "\n",
    "    # Live display\n",
    "    cv2.imshow('Live Crowd Headcount (CSRNet)', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"‚úÖ Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd95ce-334b-4b18-8946-e18353bec05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === 1. Define CSRNet model (reuse your existing model.py) ===\n",
    "from model import CSRNet\n",
    "\n",
    "# === 2. Load model ===\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CSRNet().to(device)\n",
    "\n",
    "checkpoint = torch.load(\"partBmodel_best.pth\", map_location=device, weights_only=False)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# === 3. Define transform ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# === 4. Patch-based CSRNet counting ===\n",
    "def count_frame_patched(frame, model, transform, patch_size=(384, 384)):\n",
    "    H, W, _ = frame.shape\n",
    "    patch_h, patch_w = patch_size\n",
    "    total_count = 0\n",
    "\n",
    "    for y in range(0, H, patch_h):\n",
    "        for x in range(0, W, patch_w):\n",
    "            patch = frame[y:min(y+patch_h, H), x:min(x+patch_w, W)]\n",
    "            patch_resized = cv2.resize(patch, patch_size)\n",
    "            patch_rgb = cv2.cvtColor(patch_resized, cv2.COLOR_BGR2RGB)\n",
    "            patch_pil = Image.fromarray(patch_rgb)\n",
    "            input_tensor = transform(patch_pil).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                density_map = model(input_tensor)\n",
    "                count = float(density_map.sum().item())\n",
    "                total_count += count\n",
    "\n",
    "    return int(total_count)\n",
    "\n",
    "# === 5. Ask user for input source ===\n",
    "print(\"üîò Choose input source:\")\n",
    "print(\"1: Local video file\")\n",
    "print(\"2: Live CCTV stream (RTSP link)\")\n",
    "print(\"3: Webcam\")\n",
    "choice = input(\"Enter your choice (1/2/3): \").strip()\n",
    "\n",
    "if choice == \"1\":\n",
    "    input_path = input(\"Enter the full path of the video file: \").strip().strip('\"')\n",
    "elif choice == \"2\":\n",
    "    input_path = input(\"Enter the RTSP link of the CCTV stream: \").strip()\n",
    "elif choice == \"3\":\n",
    "    input_path = 0  # Default webcam\n",
    "else:\n",
    "    print(\"‚ùå Invalid choice.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# === 6. Process and display ===\n",
    "print(\"üîç Live processing... Press ESC to quit.\")\n",
    "\n",
    "frame_skip = 5\n",
    "\n",
    "for i in tqdm(range(0, frame_count, frame_skip)):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "    count = count_frame_patched(frame, model, transform)\n",
    "\n",
    "    # Overlay count\n",
    "    cv2.putText(frame, f\"Count: {count}\", (30, 60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.8, (0, 0, 255), 3)\n",
    "\n",
    "    # Live display\n",
    "    cv2.imshow('Live Crowd Headcount (CSRNet)', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"‚úÖ Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (csrnet_env)",
   "language": "python",
   "name": "csrnet_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
